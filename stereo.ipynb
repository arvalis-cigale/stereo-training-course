{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b29a189-c4e0-42fd-adc8-e9a11b88764c",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <div style=\"font-size: 40px; color: black\"><b>Extraction of 3D information from binocular images</b></div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7af6d-d575-43ad-a0f0-bd13a3d44e6d",
   "metadata": {},
   "source": [
    "EPPS Training School 12.09.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfc1dd-44e0-40fe-a011-3051c98bac02",
   "metadata": {},
   "source": [
    "Samuel Thomas - R&D signal & image processing engineer - team Cigale, Arvalis (s.thomas@arvalis.fr)\n",
    "Part of the LPA CAPTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abcb61-7119-4b5f-8378-e765ec71e7fc",
   "metadata": {},
   "source": [
    "licensed under CC By NC 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2020b-a93a-4e54-a525-118ab652bc1f",
   "metadata": {},
   "source": [
    "# Part 2 : 3D reconstruction of a scene from a stereo pair of images\n",
    "\n",
    "In this notebook, you will find the code with the main required steps for making a depth map, and a simple estimation of the height "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d4283-b13c-4299-8774-4dbdcea454f0",
   "metadata": {},
   "source": [
    "Documentation may be found here :\n",
    "\n",
    "https://docs.opencv.org/4.x/d9/db7/tutorial_py_table_of_contents_calib3d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a299b-121b-4727-aada-618372bcdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import stereolib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be775a2-9a7f-4d9f-b8ad-be90d548177e",
   "metadata": {},
   "source": [
    "open3d is a project described here :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a28f2a-0166-45a2-aa0d-e480edfcad15",
   "metadata": {},
   "source": [
    "@article{Zhou2018,\n",
    "    author    = {Qian-Yi Zhou and Jaesik Park and Vladlen Koltun},\n",
    "    title     = {{Open3D}: {A} Modern Library for {3D} Data Processing},\n",
    "    journal   = {arXiv:1801.09847},\n",
    "    year      = {2018},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5c035-dd62-4256-8683-a9c2ea622338",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753de65b-19fc-4d90-ace6-b65409d719e8",
   "metadata": {},
   "source": [
    "## 1.  Loading the calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a65a5-f79b-442a-8036-1c1af1f065fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data directory\n",
    "path_data = Path(\"./data\")\n",
    "\n",
    "# Load calibration matrices and vectors\n",
    "calibration_path = path_data / Path(\"calibration\") / Path(\"literal_v2\")\n",
    "\n",
    "# Calibration path for Literal v2    \n",
    "calibration_filename = calibration_path / \"Tricam.yaml\"\n",
    "\n",
    "# If this file exists, then load calibration from its content\n",
    "if os.path.exists(calibration_filename):\n",
    "\n",
    "    img_size_left, camera_mtx_left, dist_coefs_left, \\\n",
    "    img_size_right, camera_mtx_right, dist_coefs_right, \\\n",
    "    R_pair, T_pair, \\\n",
    "    serial_number, \\\n",
    "    calibration_datetime = stereolib.load_calibration_from_yaml(calibration_filename)\n",
    "\n",
    "# Calibration has to be loaded from files previously generated with calibration.ipynb, from chessboard images\n",
    "else:\n",
    "    \n",
    "    img_size_left, camera_mtx_left, dist_coefs_left, \\\n",
    "    img_size_right, camera_mtx_right, dist_coefs_right, \\\n",
    "    R_pair, T_pair = stereolib.load_calibration_from_numpy(calibration_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25beb43-5970-491e-8a61-51eadeb28bff",
   "metadata": {},
   "source": [
    "## 2.  Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d6c46f-243b-4106-b132-c82f304ef922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for our input images\n",
    "path_rgb_images = path_data / Path(\"inputs\")\n",
    "\n",
    "# Elements composing the name of our images\n",
    "name_base_camera = \"Tricam1Camera\"\n",
    "left_id = 1\n",
    "right_id = 2\n",
    "plot_id = \"556-A\"\n",
    "image_id = \"1\"\n",
    "image_ext = \"png\"\n",
    "\n",
    "# Name of the pair of images used\n",
    "name_img_left = path_rgb_images / f\"Plot{plot_id}_{name_base_camera}{left_id}_{image_id}.{image_ext}\"\n",
    "name_img_right = path_rgb_images / f\"Plot{plot_id}_{name_base_camera}{right_id}_{image_id}.{image_ext}\"\n",
    "\n",
    "# Load left and right input images\n",
    "img_left = stereolib.read_image(name_img_left)\n",
    "img_right = stereolib.read_image(name_img_right)\n",
    "\n",
    "# Paths for generated outputs\n",
    "path_outputs = path_data / Path(\"outputs\")\n",
    "if os.path.isdir(path_outputs) is False:\n",
    "            os.mkdir(path_outputs)\n",
    "\n",
    "# Display input images\n",
    "stereolib.display_images([name_img_left, name_img_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babaa46-e7b1-404c-8dc8-6e17a1687136",
   "metadata": {},
   "source": [
    "## Step 1. Rectifying the images\n",
    "\n",
    "This could be done during the calibration process, but as those maps are big matrices (in our case, from 30 to 60 Mb each), we prefer doing this at the beggining of the reconstruction part, as it's done once and does not take too much time.\n",
    "\n",
    "Reminder : starting with the original 'raw' images, the objective of this stage is to get rectified left and right images, i.e. the left and right projective planes are coplanar and their rows are perfectly aligned :\n",
    "\n",
    "<img src=\"static/stereo_rectification.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa33bb9-2069-47ff-85a9-a1838c4ae205",
   "metadata": {},
   "source": [
    "## Generating the rectification maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123e86d-e856-4b7b-8714-4d5491d61e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the transformation matrices for rectification, from the intrinsic and distorsion parameters of each camera, \n",
    "# and rotation and translation matrices describing related positions of left and right cameras (coming from stereo calibration)\n",
    "R_rect_left, R_rect_right, \\\n",
    "P_rect_left, P_rect_right, \\\n",
    "Q_rect_left, \\\n",
    "valid_pix_roi_1, valid_pix_roi_2  = cv.stereoRectify(camera_mtx_left,\n",
    "                                                     dist_coefs_left,\n",
    "                                                     camera_mtx_right,\n",
    "                                                     dist_coefs_right,\n",
    "                                                     img_size_left,\n",
    "                                                     R_pair,\n",
    "                                                     T_pair)\n",
    "\n",
    "# We compute the maps (offset along rows and columns) to be applied to each pixel of each camera image (left and right), so that the left and right images\n",
    "# are rectified (= lines of the two cameras are aligned)\n",
    "# Left camera\n",
    "map_left_1, map_left_2 = cv.initUndistortRectifyMap(camera_mtx_left,\n",
    "                                                    dist_coefs_left,\n",
    "                                                    R_rect_left,\n",
    "                                                    P_rect_left,\n",
    "                                                    img_size_left,\n",
    "                                                    cv.CV_16SC2)\n",
    "\n",
    "# Right camera\n",
    "map_right_1, map_right_2 = cv.initUndistortRectifyMap(camera_mtx_right,\n",
    "                                                      dist_coefs_right,\n",
    "                                                      R_rect_right,\n",
    "                                                      P_rect_right,\n",
    "                                                      img_size_right,\n",
    "                                                      cv.CV_16SC2)\n",
    "\n",
    "# Distance between the two cameras, in mm. Expressed in mm, because at the calibration step, we specified the chessboard cell size in millimeters.\n",
    "# This is simply computed considering the norm of the translation vector resulting of the stereo calibration pair.\n",
    "baseline = np.linalg.norm(T_pair)\n",
    "\n",
    "print(f\"baseline = {baseline} mm\")\n",
    "\n",
    "# Focal length obtained after rectification, expressed in pixels \n",
    "focal_length = P_rect_left[0][0]\n",
    "\n",
    "print(f\"focal length = {focal_length} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48307461-db99-4da5-ac95-5085e391efd0",
   "metadata": {},
   "source": [
    "## Applying the rectification maps to the input image pairs\n",
    "\n",
    "At this step, we have to apply the correction maps computed for each camera at the beginning to each of our left and right input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d0bcb2-38fc-4db3-95f5-56f82d0f5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectify both images, so that the lines of the left image are matching those in the right image.\n",
    "rect_img_left = stereolib.rectify(img_left, map_left_1, map_left_2)\n",
    "rect_img_right = stereolib.rectify(img_right, map_right_1, map_right_2)\n",
    "\n",
    "# Save those images\n",
    "name_rect_img_left = path_outputs / \"rect_left_image.png\"\n",
    "name_rect_img_right = path_outputs / \"rect_right_image.png\"\n",
    "\n",
    "stereolib.write_image(name_rect_img_left, rect_img_left)\n",
    "stereolib.write_image(name_rect_img_right, rect_img_right)\n",
    "\n",
    "# Check the size of the recified image\n",
    "print(f\"Rectified left image shape = {rect_img_left.shape}\")\n",
    "\n",
    "# We store height and width of the left image for further use\n",
    "height, width = rect_img_left.shape[:2]\n",
    "\n",
    "# Display rectified images\n",
    "stereolib.display_images([name_rect_img_left, name_rect_img_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a83fb-dc0f-4b78-ad48-f3d724ebbcf1",
   "metadata": {},
   "source": [
    "In the rectification operation, we chose to keep the original image size : you can observe that both left and right images were slightly tilted, black pixels added at the bottom of the left, and at the top and left of the right image.\n",
    "You could also try to play with the parameters in stereolib.rectify and see the way in which this influences the rectified images (and the use of the ROI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc706ff-dced-44b4-bd64-348b98a0296b",
   "metadata": {},
   "source": [
    "# Step 2 : Computing the matching correspondence between the left and right images\n",
    "\n",
    "We have to deal with a lot of parameters at this step ; you could try to see the influence of these parameters on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed363772-6fb2-4758-b8e1-6539750aa27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of depth_image in which something is interesting (depending on one's application), in mm\n",
    "range_operating_depth = [1200, 2800]\n",
    "\n",
    "# We compute disparities corresponding to the given range\n",
    "min_disparity_init = int(stereolib.compute_disparity_from_depth(baseline, focal_length, range_operating_depth[1]))\n",
    "\n",
    "# Prohibits the '0' value (would lead to infinite depth_image as Z = (baseline * focal_length) / disparity)\n",
    "min_disparity_init = max(min_disparity_init, 1)\n",
    "min_disparity_init = int(min_disparity_init)\n",
    "print(f\"Min disparity boundary : {min_disparity_init}\")\n",
    "\n",
    "# Rounds to upper digit to ensure to matching the desired value\n",
    "max_disparity_init = int(stereolib.compute_disparity_from_depth(baseline, focal_length, range_operating_depth[0])) + 1\n",
    "print(f\"Max disparity boundary : {max_disparity_init}\")\n",
    "\n",
    "# We compute the number of disparities (must be a multiple of 16)\n",
    "num_disparities_init = stereolib.compute_number_of_disparities(min_disparity_init, max_disparity_init)\n",
    "print(f\"Number of disparities : {num_disparities_init}\")\n",
    "\n",
    "# Convert RGB images to grayscale for matching (not mandatory, may work with RGB images, but more demanding in terms of computation power and of quality of the input images)\n",
    "rect_gray_img_left = cv.cvtColor(rect_img_left, cv.COLOR_BGR2GRAY)\n",
    "rect_gray_img_right = cv.cvtColor(rect_img_right, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# As images are rectified and both lines aligned, you may choose to downsample images along lines to reduce processing time and CPU charge\n",
    "# (dependending on the resolution and objects size)\n",
    "# Here for example we downsample by a 4 factor\n",
    "factor_dwns = 4\n",
    "rect_gray_img_left_dwns = np.copy(rect_gray_img_left[::factor_dwns, ...])\n",
    "rect_gray_img_right_dwns = np.copy(rect_gray_img_right[::factor_dwns, ...])\n",
    "\n",
    "# Parameters for filtering the disparity (/!\\ : good rendering but may lead to approximations in distances). We don't activate filtering by default.\n",
    "wls_activation = False\n",
    "wls_sigma = 1.2\n",
    "\n",
    "# We compute the disparity using for SGBM algorithm, with efficient parameters (for our application and from our experience)\n",
    "ret, disparity_map_dwns, min_disparity, max_disparity = stereolib.stereo_processing(rect_gray_img_left_dwns,\n",
    "                                                                                    rect_gray_img_right_dwns,\n",
    "                                                                                    Q_rect_left,\n",
    "                                                                                    min_disparity_init,\n",
    "                                                                                    num_disparities_init,\n",
    "                                                                                    block_size=3,\n",
    "                                                                                    uniqueness_ratio=1,\n",
    "                                                                                    wls_activation=False,\n",
    "                                                                                    wls_sigma=1.2)\n",
    "\n",
    "# As (if) downsampling has been done, we have to resize and interpolate disparity map to original RGB images size\n",
    "disparity_map = cv.resize(disparity_map_dwns, (width, height), cv.INTER_LINEAR_EXACT)\n",
    "\n",
    "disparity_image = stereolib.normalize_image(disparity_map)\n",
    "\n",
    "name_disparity_image = path_outputs / \"disparity.png\"\n",
    "stereolib.write_image(name_disparity_image, disparity_image)\n",
    "\n",
    "stereolib.display_images([name_disparity_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653377c-ffa6-4170-82d3-6147b7e3ecc1",
   "metadata": {},
   "source": [
    "# Step 3 : Evaluating the matching quality by computing the Normalized Total Gradient (NTG) \n",
    "\n",
    "We compute the normalized total gradient to evaluate the matching quality : the principle is to compare and the left and the right-on-left mapped image obtained by applying disparity to each pixel.\n",
    "The ntg value is normalized in the [0,1] interval ; a lower value means a good matching (generally around 0.2 / 0.3) and a higher one (above 0.5) a bad disparity map quality\n",
    "\n",
    "- Disparity maps on maize and associated NTG :\n",
    "\n",
    "<img src=\"static/ntg_maize.png\" width=\"500\"/>\n",
    "\n",
    "- Disparity maps on wheat and associated NTG :\n",
    "\n",
    "<img src=\"static/ntg_wheat.png\" width=\"750\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e407a6-33d4-4347-b152-2e3afe74dc4c",
   "metadata": {},
   "source": [
    "@article{\n",
    "    author    = {Chen, Shu-Jie & Shen, Hui-Liang & Li, Chunguang & Xin, John.},\n",
    "    title     = {Normalized Total Gradient: A New Measure for Multispectral Image Registration.},\n",
    "    journal   = {IEEE Transactions on Image Processing. PP. 1-1. 10.1109/TIP.2017.2776753.},\n",
    "    year      = {2017},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007e016-78e5-4845-8d06-a269dd6e6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute NTG with donwsampled data, so that it is (much) faster.\n",
    "# It underestimates a bit the value, but the trends are conserved.\n",
    "ntg_value, difference_img, right_on_left = stereolib.normalized_total_gradient(rect_gray_img_left_dwns,\n",
    "                                                                               disparity_map_dwns,\n",
    "                                                                               rect_gray_img_right_dwns,\n",
    "                                                                               min_disparity=min_disparity,\n",
    "                                                                               max_disparity=max_disparity,\n",
    "                                                                              )\n",
    "\n",
    "print(f\"NTG = {ntg_value}\")\n",
    "\n",
    "# As (if) downsampling has been done, we have to resize and interpolate disparity map to original RGB images size\n",
    "difference_img = cv.resize(difference_img, (width-max_disparity, height), cv.INTER_LINEAR_EXACT)\n",
    "right_on_left = cv.resize(right_on_left, (width-max_disparity, height), cv.INTER_LINEAR_EXACT)\n",
    "\n",
    "\n",
    "# Save those images\n",
    "name_left_img = path_outputs / \"ntg_left_image.png\"\n",
    "name_difference_img = path_outputs / \"ntg_difference_image.png\"\n",
    "name_right_on_left_img = path_outputs / \"ntg_right_on_left_image.png\"\n",
    "\n",
    "stereolib.write_image(name_left_img, rect_gray_img_left[:, max_disparity:])\n",
    "stereolib.write_image(name_difference_img, difference_img)\n",
    "stereolib.write_image(name_right_on_left_img, right_on_left)\n",
    "\n",
    "stereolib.display_images([name_left_img, name_right_on_left_img, name_difference_img])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccc6bc-b048-4e2d-a3b6-f8a3881d9902",
   "metadata": {},
   "source": [
    "# Step 4 : Reprojecting the image pixels to 3D coordinates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05a116-0f2f-458c-9486-cd01340d99b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixels for which no correspondence has been found are set to 'min_disparity-1' by the SGBM algorithm.\n",
    "# We set these pixels to a negative value for easier discrimination (corresponding distances will be <0 too)\n",
    "\n",
    "disparity_map[disparity_map == min_disparity - 1] = -1\n",
    "\n",
    "# 3 channels image containing (X, Y, Z) coordinates in mm of each pixel (in the rectified left reference frame)\n",
    "xyz_image = cv.reprojectImageTo3D(disparity_map, Q_rect_left, handleMissingValues=False)\n",
    "\n",
    "# Crop the left stripe corresponding to max disparity (closest pixels not seen in the right image) in the 3 images that need to be coregistered\n",
    "# Here you could choose not to work on the full image, but on a central cropped zone for example, or on a masked region (if you have a vegetation\n",
    "# segmentation mask generated from the RGB left image for example ; in this case, be careful to correctly rectify your mask as was done with the\n",
    "# left RGB image, so that it's correctly coregistered with your left RGB image)\n",
    "rect_img_left = rect_img_left[:, max_disparity:]\n",
    "disparity_map = disparity_map[:, max_disparity:]\n",
    "xyz_image = xyz_image[:, max_disparity:]\n",
    "\n",
    "# We get the Z coordinates image and store it in a new variable\n",
    "depth_image = xyz_image[:,:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475180ef-6e5e-441a-a51a-a335de69ade8",
   "metadata": {},
   "source": [
    "# Step 5 : Filtering the depth image by the 3D approach\n",
    "\n",
    "At this step, we propose to take advantage of the 3D information we get to filter the imperfections that were generated at the stereo matching step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f33a87-8f3d-4c10-8172-ba2187035150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(depth_image) - np.min(depth_image)) / 10.)\n",
    "\n",
    "# We compute the full depth_array histogram, without any filtering\n",
    "depth_hist, depth_bins = np.histogram(depth_image, bins=hist_range)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(depth_bins[1:], depth_hist)\n",
    "\n",
    "# We compute the depth_image histogram, without the Z negative erratic values (corresponding to pixels \n",
    "# for which no correspondence was found between left and right images)\n",
    "depth_array = xyz_image[:,:,2][xyz_image[:,:,2] >= 0]\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(depth_array) - np.min(depth_array)) / 10.)\n",
    "\n",
    "depth_hist, depth_bins = np.histogram(depth_array, bins=hist_range)\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(depth_bins[1:], depth_hist)\n",
    "\n",
    "# To remove erratic reamining points, that may lead to bad soil distance estimation or plant height estimation, we will work with the 3D point cloud\n",
    "# for an optimal filtering efficiency\n",
    "\n",
    "# Copy the original (X,Y,Z) point cloud before downsampling\n",
    "xyz_dwn_filtered = (np.copy(xyz_image).reshape(-1, 3).astype(np.float64))\n",
    "\n",
    "# Keep only the valid Z values, i.e. positive ones (negative values dued to invalid disparity set to -1\n",
    "valid_z_values, = np.where(xyz_dwn_filtered[:, 2] >= 0)\n",
    "\n",
    "# Copy the original (R,G,B) point cloud before downsampling\n",
    "rgb_left_dwn_filtered = (np.copy(rect_img_left).reshape(-1, 3).astype(np.float64))\n",
    "\n",
    "# Convert 3 channels (X,Y,Z) image into a Open3D point cloud \n",
    "pcd_xyz = o3d.geometry.PointCloud()\n",
    "pcd_xyz.points = o3d.utility.Vector3dVector(xyz_dwn_filtered[valid_z_values])\n",
    "\n",
    "# Convert 3 channels (R,G,B) image into a Open3D point cloud \n",
    "pcd_rgb = o3d.geometry.PointCloud()\n",
    "pcd_rgb.points = o3d.utility.Vector3dVector(rgb_left_dwn_filtered[valid_z_values])\n",
    "\n",
    "# Downsampling factor for the point cloud (saves time and memory)\n",
    "ptcld_downsampling_factor = 12\n",
    "\n",
    "# Downsample original (X,Y,Z) point cloud for computation gain\n",
    "pcd_xyz_dwn = pcd_xyz.uniform_down_sample(every_k_points=ptcld_downsampling_factor)\n",
    "\n",
    "# Downsample corresponding (R,G,B) point cloud the same way\n",
    "pcd_rgb_dwn = pcd_rgb.uniform_down_sample(every_k_points=ptcld_downsampling_factor)\n",
    "\n",
    "# Convert the (X,Y,Z) point cloud and corresponding (R,G,B) point cloud into classical numpy arrays for filtering\n",
    "xyz_dwn_filtered = np.array(pcd_xyz_dwn.points)\n",
    "rgb_left_dwn_filtered = np.array(pcd_rgb_dwn.points)\n",
    "\n",
    "# Nb of neighbours for filtering\n",
    "nb_of_neighbors = 256\n",
    "# Standard deviation for filtering\n",
    "std_ratio = 0.05\n",
    "\n",
    "# Filter X,Y,Z point cloud ; returns the filtered X,Y,Z point cloud and the vector containing the indices of remaining valid points \n",
    "# in the original point cloud\n",
    "pcd_filtered, valid_pixels_array = pcd_xyz_dwn.remove_statistical_outlier(nb_neighbors=nb_of_neighbors, std_ratio=std_ratio)\n",
    "\n",
    "# Keep only the valid points in the (X,Y,Z) and corresponding (R,G,B) point cloud\n",
    "xyz_dwn_filtered = xyz_dwn_filtered[valid_pixels_array]\n",
    "rgb_left_dwn_filtered = rgb_left_dwn_filtered[valid_pixels_array]\n",
    "\n",
    "# Name of the point cloud to be written\n",
    "point_cloud_name = path_outputs / \"pointcloud.laz\"\n",
    "\n",
    "# xyz_dwn_filtered = xyz_dwn_filtered[veg_mask_dwn_filtered>0]\n",
    "# rgb_left_dwn_filtered = rgb_left_dwn_filtered[veg_mask_dwn_filtered>0]\n",
    "stereolib.write_point_cloud(point_cloud_name, xyz_dwn_filtered, rgb_left_dwn_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f0856-4116-417c-bf35-a8cfdaf46713",
   "metadata": {},
   "source": [
    "### Outputs :\n",
    "\n",
    "Here is the illustration of the efficiency of the 3D filtering\n",
    "\n",
    "<img src=\"static/filtered_pointcloud.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bda6f96-f087-4b4e-a4c3-c6a10373a843",
   "metadata": {},
   "source": [
    "# Step 6 : Estimating the mean height of the plants with a basic method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0d871-1b17-40ee-a52a-6f5910167712",
   "metadata": {},
   "source": [
    "## 1.  Estimation of the ground depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1f3d5-a90b-4ae8-961b-ac6ff83339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the third channel of the (X,Y,Z) filtered downsampled array (float32 values, in mm), corresponding to z values, i.e. depth_image to sensor\n",
    "depth_array = xyz_dwn_filtered[:, 2]\n",
    "\n",
    "# Convert the floating values into rounded integer values (mm precision is more than what is reasonably achievable).\n",
    "depth_array = np.round(depth_array).astype(np.uint32)\n",
    "\n",
    "# Plot the depth_array histogram\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(depth_array) - np.min(depth_array)) / 10.)\n",
    "\n",
    "# We compute the depth_array histogram \n",
    "depth_hist, depth_bins = np.histogram(depth_array, bins=hist_range)\n",
    "#depth_hist, depth_bins = np.histogram(depth_array, bins=10)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution after 3D filtering\")\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(depth_bins[1:], depth_hist)\n",
    "\n",
    "# Smooth the histogram to remove micropeaks\n",
    "filtered_depth_hist = gaussian_filter1d(depth_hist, sigma=2)\n",
    "\n",
    "# We normalize the histogram\n",
    "norm_filtered_depth_hist = filtered_depth_hist / np.max(filtered_depth_hist)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution after a light smoothing\")\n",
    "\n",
    "# We plot the filtered depth histogram\n",
    "plt.plot(depth_bins[1:], norm_filtered_depth_hist)\n",
    "#plt.savefig(path_outputs / Path('filtered_depth_hist.png'))\n",
    "\n",
    "# We look for peaks into the depth histogram. In the favourable case (such this one) where soil is not so masked by the vegetation, \n",
    "# we can assume that the last peak corresponds to the ground level.\n",
    "# The function returns peaks indices in the array, if any was found\n",
    "peaks, _ = find_peaks(norm_filtered_depth_hist, height=0.1)\n",
    "\n",
    "# We display the depths corresponding to the peaks indices\n",
    "depth_bins[peaks+1]\n",
    "\n",
    "# We assign the depth of the last peak to our ground estimated value\n",
    "ground_depth = 0\n",
    "if len(peaks) > 0:\n",
    "    ground_depth = depth_bins[peaks[-1]+1]\n",
    "\n",
    "ground_depth = np.round(ground_depth).astype(np.uint32)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# We plot once again the filtered depth histogram\n",
    "plt.plot(depth_bins[1:], norm_filtered_depth_hist)\n",
    "\n",
    "# Margins and limits of the plot\n",
    "ax.margins(x=0, y=0)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# We plot the vertical line corresponding to the ground estimated depth\n",
    "plt.vlines(ground_depth, 0,1, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'{ground_depth}', xy=(ground_depth, 0), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution and ground depth estimation\")\n",
    "\n",
    "plt.savefig(path_outputs / Path('filtered_depth_hist.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f164b79-5cfa-4426-a91a-9089f2af1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ground depth estimation = {(ground_depth)} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936bf60-55ee-4211-9c3a-c8b1defe46c3",
   "metadata": {},
   "source": [
    "### Checking\n",
    "\n",
    "Let's have a look at the information we can retrieve picking a soil point in our pointcloud \n",
    "(Made with CloudCompare)\n",
    "\n",
    "<img src=\"static/read_soil.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba45f55-e463-46b1-8bb9-820d2d52575c",
   "metadata": {},
   "source": [
    "## 2.  Estimation of the mean height of the plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67abb6-7ef9-4e41-9556-8f9ac9e0d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heigths may then be simply estimated by calculating the difference between the ground distance and the distances to objects\n",
    "height_array = ground_depth - depth_array\n",
    "\n",
    "# We ensure that there won't be values out of the expected range\n",
    "height_array = height_array[height_array > 0]\n",
    "height_array = height_array[height_array < ground_depth]\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(height_array) - np.min(height_array)) / 10.)\n",
    "\n",
    "# We compute the height histogram\n",
    "height_hist, height_bins = np.histogram(height_array, bins=hist_range)\n",
    "\n",
    "# Smooth the histogram to remove micropeaks, just as we did with depth\n",
    "filtered_height_hist = gaussian_filter1d(height_hist, sigma=2)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Height above ground_depth in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Height distribution\")\n",
    "\n",
    "# We plot the histogram\n",
    "plt.plot(height_bins[1:], filtered_height_hist)\n",
    "plt.savefig(path_outputs / Path('filtered_height_hist.png'))\n",
    "\n",
    "# We compute the cumulative height distribution\n",
    "cumulative_height_distribution = np.cumsum(filtered_height_hist)\n",
    "\n",
    "# We normalize it so that valus are int the interval [0,1]\n",
    "cumulative_height_distribution = cumulative_height_distribution / np.max(cumulative_height_distribution)\n",
    "\n",
    "# We define the desired percentile of the distribution, that will lead to a consistent height value (for wheat), \n",
    "# based on many comparisons with manual height measurements.\n",
    "percentile = 0.98\n",
    "\n",
    "# We keep only height values that are above this percentile\n",
    "mean_height_array, = np.where(cumulative_height_distribution >= percentile)\n",
    "\n",
    "# We retrieve the first value and define it as the mean height\n",
    "mean_height = height_bins[mean_height_array[0]+1]\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# We plot the cumulative heights histogram\n",
    "plt.plot(height_bins[1:], cumulative_height_distribution)\n",
    "\n",
    "# We plot the horizontal line corresponding to the desired percentile (98%) of the the height cumulative distribution count \n",
    "plt.hlines(0.98, 0, mean_height, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'0.98', xy=(0, 0.98), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# We plot the vertical line where the desired percentile (98%) is reached in the cumulative distribution\n",
    "plt.vlines(mean_height, 0, 0.98, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'{mean_height:.1f}', xy=(mean_height, 0), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Margins and limits of the plot\n",
    "ax.margins(x=0, y=0)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Heights in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Cumulative heights distribution\")\n",
    "\n",
    "plt.savefig(path_outputs / Path('cumulative_height_distribution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba83c-b060-43c7-a621-cf89f0c52a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Plant mean height estimation = {int(mean_height)} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ff3fc-c9d0-443b-bbed-555bf6655977",
   "metadata": {},
   "source": [
    "### Checking\n",
    "\n",
    "Let's have a look at the information we can retrieve picking a soil point in our pointcloud \n",
    "(Made with CloudCompare)\n",
    "\n",
    "<img src=\"static/read_height.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f6a14-bacc-4e41-928c-fae4590d1a4b",
   "metadata": {},
   "source": [
    "### Generating the height image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af136109-6b44-4085-8b9f-436297ca0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can go back to the non-filtered depth image to have a representation of the heights of objects in the image, \n",
    "# Distances are in mm in depth_image ; we divide those values by 10, so that we get the heights with a cm precision in a 8 bits image\n",
    "# (values between 0 and 255 cm)\n",
    "height_image = ((ground_depth - depth_image) / 10.)\n",
    "\n",
    "height_image[height_image < 0] = 0\n",
    "height_image[height_image > 255] = 0 \n",
    "\n",
    "height_image = height_image.astype(np.uint8)\n",
    "\n",
    "name_height_image = path_outputs / \"height.png\"\n",
    "\n",
    "stereolib.write_image(name_height_image, height_image)\n",
    "\n",
    "stereolib.display_images([name_height_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac26223-eef1-449d-84df-75f79e081311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
