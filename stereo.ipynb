{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b29a189-c4e0-42fd-adc8-e9a11b88764c",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <div style=\"font-size: 40px; color: black\"><b>Extraction of 3D information from binocular images</b></div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7af6d-d575-43ad-a0f0-bd13a3d44e6d",
   "metadata": {},
   "source": [
    "EPPS Training School 12.09.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfc1dd-44e0-40fe-a011-3051c98bac02",
   "metadata": {},
   "source": [
    "Samuel Thomas - R&D signal & image processing engineer - team Cigale, Arvalis (s.thomas@arvalis.fr)\n",
    "Part of the LPA CAPTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abcb61-7119-4b5f-8378-e765ec71e7fc",
   "metadata": {},
   "source": [
    "licensed under CC By NC 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e2020b-a93a-4e54-a525-118ab652bc1f",
   "metadata": {},
   "source": [
    "# Part 2 : 3D reconstruction of a scene from a stereo pair of images\n",
    "\n",
    "In this notebook, you will find the code with the main required steps for making a depth map, and a simple estimation of the height "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d4283-b13c-4299-8774-4dbdcea454f0",
   "metadata": {},
   "source": [
    "### ðŸ“Œ Documentation may be found here :\n",
    "\n",
    "From OpenCV :\n",
    "\n",
    "[stereoRectify](./documentation/OpenCV_stereoRectify.pdf)\n",
    "\n",
    "[initUndistortRectifyMap](./documentation/OpenCV_initUndistortRectifyMap.pdf)\n",
    "\n",
    "[remap](./documentation/OpenCV_remap.pdf)\n",
    "\n",
    "[StereoSGBM](./documentation/OpenCV_StereoSGBM_create.pdf)\n",
    "\n",
    "https://docs.opencv.org/4.x/d9/db7/tutorial_py_table_of_contents_calib3d.html\n",
    "\n",
    "https://learnopencv.com/depth-perception-using-stereo-camera-python-c/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085a299b-121b-4727-aada-618372bcdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import open3d as o3d\n",
    "import laspy\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import stereolib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be775a2-9a7f-4d9f-b8ad-be90d548177e",
   "metadata": {},
   "source": [
    "### ðŸ“Œ References to the open3d project (Zhou2018) :\n",
    "\n",
    "&nbsp;\n",
    "Authors : Qian-Yi Zhou and Jaesik Park and Vladlen Koltun. (2018)\n",
    "\n",
    "&nbsp;\n",
    "Title : Open3D : A Modern Library for 3D Data Processing\n",
    "\n",
    "&nbsp;\n",
    "Journal : arXiv:1801.09847"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5c035-dd62-4256-8683-a9c2ea622338",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753de65b-19fc-4d90-ace6-b65409d719e8",
   "metadata": {},
   "source": [
    "## 1.  Loading the calibration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a65a5-f79b-442a-8036-1c1af1f065fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data directory\n",
    "path_data = Path(\"./data\")\n",
    "\n",
    "# Load calibration matrices and vectors\n",
    "calibration_path = path_data / Path(\"calibration\") / Path(\"literal_v2\")\n",
    "\n",
    "# Calibration path for Literal v2    \n",
    "calibration_filename = calibration_path / \"Tricam.yaml\"\n",
    "\n",
    "# If this file exists, then load calibration from its content\n",
    "if os.path.exists(calibration_filename):\n",
    "\n",
    "    img_size_left, camera_mtx_left, dist_coefs_left, \\\n",
    "    img_size_right, camera_mtx_right, dist_coefs_right, \\\n",
    "    R_pair, T_pair, \\\n",
    "    serial_number, \\\n",
    "    calibration_datetime = stereolib.load_calibration_from_yaml(calibration_filename)\n",
    "\n",
    "# Calibration has to be loaded from files previously generated with calibration.ipynb, from chessboard images\n",
    "else:\n",
    "    \n",
    "    img_size_left, camera_mtx_left, dist_coefs_left, \\\n",
    "    img_size_right, camera_mtx_right, dist_coefs_right, \\\n",
    "    R_pair, T_pair = stereolib.load_calibration_from_numpy(calibration_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25beb43-5970-491e-8a61-51eadeb28bff",
   "metadata": {},
   "source": [
    "## 2.  Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9118c3c2-b9f5-4916-885c-7174b9c079ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for our input images\n",
    "path_rgb_images = path_data / Path(\"exercises/inputs\")\n",
    "\n",
    "# Elements composing the name of our images\n",
    "name_base_camera = \"Tricam1Camera\"\n",
    "left_id = 1\n",
    "right_id = 2\n",
    "plot_id = \"556-A\"\n",
    "image_id = \"1\"\n",
    "image_ext = \"png\"\n",
    "\n",
    "# Name of the pair of images used\n",
    "name_img_left = path_rgb_images / f\"Plot{plot_id}_{name_base_camera}{left_id}_{image_id}.{image_ext}\"\n",
    "name_img_right = path_rgb_images / f\"Plot{plot_id}_{name_base_camera}{right_id}_{image_id}.{image_ext}\"\n",
    "\n",
    "# Load left and right input images\n",
    "img_left = stereolib.read_image(name_img_left)\n",
    "img_right = stereolib.read_image(name_img_right)\n",
    "\n",
    "# Paths for generated outputs\n",
    "path_outputs = path_data / Path(\"exercises/outputs\")\n",
    "if os.path.isdir(path_outputs) is False:\n",
    "            os.mkdir(path_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ca9a8-7791-41e2-8890-e2963d3a8690",
   "metadata": {},
   "source": [
    "### Input images :\n",
    "\n",
    "Left and right RGB raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8eb51-7cd8-43b4-a433-ade86ab8602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereolib.display_images([name_img_left, name_img_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7babaa46-e7b1-404c-8dc8-6e17a1687136",
   "metadata": {},
   "source": [
    "# Step 1 : Rectifying the images\n",
    "\n",
    "This could be done during the calibration process, but as those maps are big matrices (in our case, from 30 to 60 Mb each), we prefer doing this at the beggining of the reconstruction part, as it's done once and does not take too much time.\n",
    "\n",
    "Reminder : starting with the original 'raw' images, the objective of this stage is to get rectified left and right images, i.e. the left and right projective planes are coplanar and their rows are perfectly aligned :\n",
    "\n",
    "<img src=\"static/stereo_rectification.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa33bb9-2069-47ff-85a9-a1838c4ae205",
   "metadata": {},
   "source": [
    "## 1.  Generating the rectification maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b529ea6d-3954-4048-b845-d18fcaba8803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the transformation matrices for rectification, from the intrinsic and distorsion parameters of each camera, \n",
    "# and rotation and translation matrices describing related positions of left and right cameras (coming from stereo calibration)\n",
    "R_rect_left, R_rect_right, \\\n",
    "P_rect_left, P_rect_right, \\\n",
    "Q_rect_left, \\\n",
    "valid_pix_roi_1, valid_pix_roi_2  = cv.stereoRectify(camera_mtx_left,\n",
    "                                                     dist_coefs_left,\n",
    "                                                     camera_mtx_right,\n",
    "                                                     dist_coefs_right,\n",
    "                                                     img_size_left,\n",
    "                                                     R_pair,\n",
    "                                                     T_pair)\n",
    "\n",
    "# We compute the maps (offset along rows and columns) to be applied to each pixel of each camera image (left and right), so that the left and right images\n",
    "# are rectified (= lines of the two cameras are aligned)\n",
    "# Left camera\n",
    "map_left_1, map_left_2 = cv.initUndistortRectifyMap(camera_mtx_left,\n",
    "                                                    dist_coefs_left,\n",
    "                                                    R_rect_left,\n",
    "                                                    P_rect_left,\n",
    "                                                    img_size_left,\n",
    "                                                    cv.CV_16SC2)\n",
    "\n",
    "# Right camera\n",
    "map_right_1, map_right_2 = cv.initUndistortRectifyMap(camera_mtx_right,\n",
    "                                                      dist_coefs_right,\n",
    "                                                      R_rect_right,\n",
    "                                                      P_rect_right,\n",
    "                                                      img_size_right,\n",
    "                                                      cv.CV_16SC2)\n",
    "\n",
    "# Distance between the two cameras, in mm. Expressed in mm, because at the calibration step, we specified the chessboard cell size in millimeters.\n",
    "# This is simply computed considering the norm of the translation vector resulting of the stereo calibration pair.\n",
    "baseline = np.linalg.norm(T_pair)\n",
    "\n",
    "# Focal length obtained after rectification, expressed in pixels \n",
    "focal_length = P_rect_left[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7458fc45-b5d6-428c-9e8c-4002aac8fe51",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Outputs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5162a-b4b6-428f-b58f-27a5f36e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"baseline = {baseline} mm\")\n",
    "print(f\"focal length = {focal_length} pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48307461-db99-4da5-ac95-5085e391efd0",
   "metadata": {},
   "source": [
    "## 2.  Applying the rectification maps to the input image pairs\n",
    "\n",
    "At this step, we have to apply the correction maps computed for each camera at the beginning to each of our left and right input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460bfca-f79e-4a26-bbf5-d2a6d949f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rectify both images, so that the lines of the left image are matching those in the right image.\n",
    "rect_img_left = stereolib.rectify(img_left, map_left_1, map_left_2)\n",
    "rect_img_right = stereolib.rectify(img_right, map_right_1, map_right_2)\n",
    "\n",
    "# Save those images\n",
    "name_rect_img_left = path_outputs / \"rect_left_image.png\"\n",
    "name_rect_img_right = path_outputs / \"rect_right_image.png\"\n",
    "\n",
    "stereolib.write_image(name_rect_img_left, rect_img_left)\n",
    "stereolib.write_image(name_rect_img_right, rect_img_right)\n",
    "\n",
    "# We store height and width of the left image for further use\n",
    "height, width = rect_img_left.shape[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6e4141-efa2-44ab-8fe2-bd47107a67a0",
   "metadata": {},
   "source": [
    "### Outputs :\n",
    "\n",
    "Rectifed left and right images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0baaa25-94e4-47e6-9490-75ceeb3bebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereolib.display_images([name_rect_img_left, name_rect_img_right])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a83fb-dc0f-4b78-ad48-f3d724ebbcf1",
   "metadata": {},
   "source": [
    "*For the rectification operation, we chose to keep the original image size : you can observe that both left and right images were slightly tilted, black pixels added at the bottom of the left, and at the top and left of the right image.\n",
    "You could also try to play with the parameters in stereolib.rectify and see the way in which this influences the rectified images (and the use of the ROI)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d712ce0-afe3-492b-a824-12268a230eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rectified left image shape = {rect_img_left.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc706ff-dced-44b4-bd64-348b98a0296b",
   "metadata": {},
   "source": [
    "# Step 2 : Computing the matching correspondence between the left and right images\n",
    "\n",
    "At the end of this stage, the aim is to retrieve a disparity value for each pixel of the reference image (say the left one)\n",
    "\n",
    "The expected disparity map is a 1-channel image of the same size as the left image, in which a pixel value encodes the distance in pixels of the counterpart right pixel on the same row\n",
    "\n",
    "### The OpenCV Semi Global Block Matching (SGBM) algorithm\n",
    "- Works on rectified left and right images, RGB or grayscale (RGB is more demanding on input  ; quality but may be in this case more accurate)\n",
    "- Needs to predefine the minimal disparity and number of disparities we want to search for (i.e. the distance range of our objects in the image) => A good choice is important for relevance and computation time\n",
    "- Works by considering the neighborhood of each pixel in several directions (more accuracy/lower speed)\n",
    "\n",
    "<img src=\"static/stereo_matching.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f49df-2eeb-4886-8745-da5a3686d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of depth_image in which something is interesting (depending on one's application), in mm\n",
    "range_operating_depth = [1200, 2800]\n",
    "\n",
    "# We compute disparities corresponding to the given range\n",
    "min_disparity_init = int(stereolib.compute_disparity_from_depth(baseline, focal_length, range_operating_depth[1]))\n",
    "\n",
    "# Prohibits the '0' value (would lead to infinite depth_image as Z = (baseline * focal_length) / disparity)\n",
    "min_disparity_init = max(min_disparity_init, 1)\n",
    "min_disparity_init = int(min_disparity_init)\n",
    "\n",
    "# Rounds to upper digit to ensure to matching the desired value\n",
    "max_disparity_init = int(stereolib.compute_disparity_from_depth(baseline, focal_length, range_operating_depth[0])) + 1\n",
    "\n",
    "# We compute the number of disparities (must be a multiple of 16)\n",
    "num_disparities_init = stereolib.compute_number_of_disparities(min_disparity_init, max_disparity_init)\n",
    "\n",
    "# Convert RGB images to grayscale for matching (not mandatory, may work with RGB images, but more demanding in terms of computation power and of quality of the input images)\n",
    "rect_gray_img_left = cv.cvtColor(rect_img_left, cv.COLOR_BGR2GRAY)\n",
    "rect_gray_img_right = cv.cvtColor(rect_img_right, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# As images are rectified and both lines aligned, you may choose to downsample images along lines to reduce processing time and CPU charge\n",
    "# (dependending on the resolution and objects size)\n",
    "# Here for example we downsample by a 4 factor\n",
    "factor_dwns = 4\n",
    "rect_gray_img_left_dwns = np.copy(rect_gray_img_left[::factor_dwns, ...])\n",
    "rect_gray_img_right_dwns = np.copy(rect_gray_img_right[::factor_dwns, ...])\n",
    "\n",
    "# Parameters for filtering the disparity (/!\\ : good rendering but may lead to approximations in distances). We don't activate filtering by default.\n",
    "wls_activation = False\n",
    "wls_sigma = 1.2\n",
    "\n",
    "# We compute the disparity using for SGBM algorithm, with efficient parameters (for our application and from our experience)\n",
    "ret, disparity_map_dwns, min_disparity, max_disparity = stereolib.stereo_processing(rect_gray_img_left_dwns,\n",
    "                                                                                    rect_gray_img_right_dwns,\n",
    "                                                                                    Q_rect_left,\n",
    "                                                                                    min_disparity_init,\n",
    "                                                                                    num_disparities_init,\n",
    "                                                                                    block_size=3,\n",
    "                                                                                    uniqueness_ratio=1,\n",
    "                                                                                    wls_activation=False,\n",
    "                                                                                    wls_sigma=1.2)\n",
    "\n",
    "# As (if) downsampling has been done, we have to resize and interpolate disparity map to original RGB images size\n",
    "disparity_map = cv.resize(disparity_map_dwns, (width, height), cv.INTER_LINEAR_EXACT)\n",
    "\n",
    "disparity_image = stereolib.normalize_image(disparity_map)\n",
    "\n",
    "name_disparity_image = path_outputs / \"disparity.png\"\n",
    "stereolib.write_image(name_disparity_image, disparity_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd77929-1ae8-4228-b6bf-f25283160ab5",
   "metadata": {},
   "source": [
    "### Outputs :\n",
    "\n",
    "Disparity image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f09496-a5f5-4331-91a0-c6f2f4d74d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereolib.display_images([name_disparity_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f7db2a-92b9-4140-8794-4e3d100fb913",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Min disparity boundary : {min_disparity_init}\")\n",
    "print(f\"Max disparity boundary : {max_disparity_init}\")\n",
    "print(f\"Number of disparities : {num_disparities_init}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72eaca-65db-458e-8efd-8321a9b24b86",
   "metadata": {},
   "source": [
    "### Question : why is there a black vertical stripe at the left of the disparity image ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653377c-ffa6-4170-82d3-6147b7e3ecc1",
   "metadata": {},
   "source": [
    "# Step 3 : Evaluating the matching quality by computing the Normalized Total Gradient (NTG) \n",
    "\n",
    "### References to the NTG paper :\n",
    "\n",
    "&nbsp;\n",
    "Authors : Chen, Shu-Jie & Shen, Hui-Liang & Li, Chunguang & Xin, John. (2017). \n",
    "\n",
    "&nbsp;\n",
    "Title : Normalized Total Gradient : A New Measure for Multispectral Image Registration. \n",
    "\n",
    "&nbsp;\n",
    "Journal : IEEE Transactions on Image Processing. PP. 1-1. 10.1109/TIP.2017.2776753. \n",
    "\n",
    "\n",
    "We compute the normalized total gradient to evaluate the matching quality : the principle is to compare and the left and the right-on-left mapped image obtained by applying disparity to each pixel.\n",
    "The ntg value is normalized in the [0,1] interval ; a lower value means a good matching (generally around 0.2 / 0.3) and a higher one (above 0.5) a bad disparity map quality\n",
    "\n",
    "- Disparity maps on maize and associated NTG :\n",
    "\n",
    "<img src=\"static/ntg_maize.png\" width=\"500\"/>\n",
    "\n",
    "- Disparity maps on wheat and associated NTG :\n",
    "\n",
    "<img src=\"static/ntg_wheat.png\" width=\"750\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be2ec8-f39b-49a8-9377-ed1d8b624758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute NTG with donwsampled data, so that it is (much) faster.\n",
    "# It underestimates a bit the value, but the trends are conserved.\n",
    "ntg_value, difference_img, right_on_left = stereolib.normalized_total_gradient(rect_gray_img_left_dwns,\n",
    "                                                                               disparity_map_dwns,\n",
    "                                                                               rect_gray_img_right_dwns,\n",
    "                                                                               min_disparity=min_disparity,\n",
    "                                                                               max_disparity=max_disparity,\n",
    "                                                                              )\n",
    "\n",
    "# As (if) downsampling has been done, we have to resize and interpolate disparity map to original RGB images size\n",
    "difference_img = cv.resize(difference_img, (width-max_disparity, height), cv.INTER_LINEAR_EXACT)\n",
    "right_on_left = cv.resize(right_on_left, (width-max_disparity, height), cv.INTER_LINEAR_EXACT)\n",
    "\n",
    "\n",
    "# Save those images\n",
    "name_left_img = path_outputs / \"ntg_left_image.png\"\n",
    "name_difference_img = path_outputs / \"ntg_difference_image.png\"\n",
    "name_right_on_left_img = path_outputs / \"ntg_right_on_left_image.png\"\n",
    "\n",
    "stereolib.write_image(name_left_img, rect_gray_img_left[:, max_disparity:])\n",
    "stereolib.write_image(name_difference_img, difference_img)\n",
    "stereolib.write_image(name_right_on_left_img, right_on_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a75ea-317a-425b-9787-1ec3d351ebf9",
   "metadata": {},
   "source": [
    "### Ouptuts :\n",
    "\n",
    "Left image | right-on-left mapped image | difference image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42e240-6352-4e0f-847b-99dce5b8a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereolib.display_images([name_left_img, name_right_on_left_img, name_difference_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8251e1-f2eb-449d-95b2-d1efc0d04f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NTG = {ntg_value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22ccc6bc-b048-4e2d-a3b6-f8a3881d9902",
   "metadata": {},
   "source": [
    "# Step 4 : Reprojecting the image pixels to 3D coordinates\n",
    "\n",
    "We knew how to project a 3D point in the image 2D plane from the intrinsics matrix, but without the Z information, we were not able to perform the reverse operation :\n",
    "\n",
    "&nbsp;\n",
    "=> The projected point might be P1, or P2, or anywhere else on the optical way\n",
    "\n",
    "Associating the disparity(x,y) value (= the distance information) to a pixel p(x,y) allows to remove the ambiguity and to find its correct position P=(X,Y,Z)\n",
    "\n",
    "<img src=\"static/reprojection.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ca6aa-c785-4406-bcf1-5a5ac66402b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 channels image containing (X, Y, Z) coordinates in mm of each pixel (in the rectified left reference frame)\n",
    "xyz_image = cv.reprojectImageTo3D(disparity_map, Q_rect_left, handleMissingValues=False)\n",
    "\n",
    "# Crop the left stripe corresponding to max disparity (closest pixels not seen in the right image) in the 3 images that need to be coregistered\n",
    "# Here you could choose not to work on the full image, but on a central cropped zone for example, or on a masked region (if you have a vegetation\n",
    "# segmentation mask generated from the RGB left image for example ; in this case, be careful to correctly rectify your mask as was done with the\n",
    "# left RGB image, so that it's correctly coregistered with your left RGB image)\n",
    "rect_img_left = rect_img_left[:, max_disparity:]\n",
    "disparity_map = disparity_map[:, max_disparity:]\n",
    "xyz_image = xyz_image[:, max_disparity:]\n",
    "\n",
    "# We get the Z coordinates image and store it in a new variable\n",
    "depth_image = xyz_image[:,:,2]\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(depth_image) - np.min(depth_image)) / 10.)\n",
    "\n",
    "# We compute the full depth_array histogram, without any filtering\n",
    "depth_hist, depth_bins = np.histogram(depth_image, bins=hist_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd0ba0-0d1b-4737-899a-5232b6701f18",
   "metadata": {},
   "source": [
    "### Outputs : \n",
    "\n",
    "Depth distribution from the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d1e9c-9b92-4dd0-b305-f35c427e162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depth_bins[1:], depth_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475180ef-6e5e-441a-a51a-a335de69ade8",
   "metadata": {},
   "source": [
    "# Step 5 : Filtering the depth image by the 3D approach\n",
    "\n",
    "At this step, we propose to take advantage of the 3D information we get to filter the imperfections that were generated at the stereo matching step\n",
    "\n",
    "Using open3d,  itâ€™s easy to put the (X,Y,Z) 3-channels image obtained at the previous step, together with the RGB pixel values, as they are all coregistered in the recitifed coordinate system of the left camera\n",
    "\n",
    "Open3d gives access to several tools for filtering pointclouds, and the results are really satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f33a87-8f3d-4c10-8172-ba2187035150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove erratic remaining points that may lead to bad soil distance estimation or plant height estimation, we will work with the 3D point cloud\n",
    "# for an optimal filtering efficiency\n",
    "\n",
    "# Copy the original (X,Y,Z) point cloud before downsampling\n",
    "xyz_dwn_filtered = (np.copy(xyz_image).reshape(-1, 3).astype(np.float64))\n",
    "\n",
    "# Keep only the valid Z values, i.e. positive ones (negative values dued to invalid disparity set to -1\n",
    "\n",
    "# Pixels for which no correspondence has been found are set to 'min_disparity-1' by the SGBM algorithm.\n",
    "# We compute the corresponding depth\n",
    "infinite_depth = stereolib.compute_depth_from_disparity(baseline, focal_length, min_disparity-1)\n",
    "# OR\n",
    "# The supposed max valid depth values correspond to 'min_disparity'\n",
    "# To be more selective, we'd rather choose to remove depth values above the max depth value\n",
    "max_depth = stereolib.compute_depth_from_disparity(baseline, focal_length, min_disparity)\n",
    "# We round max_depth to the digit above to ensure we keep all the valid values and not invalid ones\n",
    "max_depth = int(max_depth) + 1\n",
    "\n",
    "# We keep only strictly inferior values to max_depth either infinite_depth\n",
    "valid_z_values, = np.where(xyz_dwn_filtered[:, 2] < infinite_depth)\n",
    "\n",
    "# Copy the original (R,G,B) point cloud before downsampling\n",
    "rgb_left_dwn_filtered = (np.copy(rect_img_left).reshape(-1, 3).astype(np.float64))\n",
    "\n",
    "# Convert 3 channels (X,Y,Z) image into a Open3D point cloud \n",
    "pcd_xyz = o3d.geometry.PointCloud()\n",
    "pcd_xyz.points = o3d.utility.Vector3dVector(xyz_dwn_filtered[valid_z_values])\n",
    "\n",
    "# Convert 3 channels (R,G,B) image into a Open3D point cloud \n",
    "pcd_rgb = o3d.geometry.PointCloud()\n",
    "pcd_rgb.points = o3d.utility.Vector3dVector(rgb_left_dwn_filtered[valid_z_values])\n",
    "\n",
    "# Downsampling factor for the point cloud (saves time and memory)\n",
    "ptcld_downsampling_factor = 12\n",
    "\n",
    "# Downsample original (X,Y,Z) point cloud for computation gain\n",
    "pcd_xyz_dwn = pcd_xyz.uniform_down_sample(every_k_points=ptcld_downsampling_factor)\n",
    "\n",
    "# Downsample corresponding (R,G,B) point cloud the same way\n",
    "pcd_rgb_dwn = pcd_rgb.uniform_down_sample(every_k_points=ptcld_downsampling_factor)\n",
    "\n",
    "# Convert the (X,Y,Z) point cloud and corresponding (R,G,B) point cloud into classical numpy arrays for filtering\n",
    "xyz_dwn_filtered = np.array(pcd_xyz_dwn.points)\n",
    "rgb_left_dwn_filtered = np.array(pcd_rgb_dwn.points)\n",
    "\n",
    "# Nb of neighbours for filtering\n",
    "nb_of_neighbors = 256\n",
    "# Standard deviation for filtering\n",
    "std_ratio = 0.05\n",
    "\n",
    "# Filter X,Y,Z point cloud ; returns the filtered X,Y,Z point cloud and the vector containing the indices of remaining valid points \n",
    "# in the original point cloud\n",
    "pcd_filtered, valid_pixels_array = pcd_xyz_dwn.remove_statistical_outlier(nb_neighbors=nb_of_neighbors, std_ratio=std_ratio)\n",
    "\n",
    "# Keep only the valid points in the (X,Y,Z) and corresponding (R,G,B) point cloud\n",
    "xyz_dwn_filtered = xyz_dwn_filtered[valid_pixels_array]\n",
    "rgb_left_dwn_filtered = rgb_left_dwn_filtered[valid_pixels_array]\n",
    "\n",
    "# Name of the point cloud to be written\n",
    "point_cloud_name = path_outputs / \"pointcloud.laz\"\n",
    "\n",
    "# xyz_dwn_filtered = xyz_dwn_filtered[veg_mask_dwn_filtered>0]\n",
    "# rgb_left_dwn_filtered = rgb_left_dwn_filtered[veg_mask_dwn_filtered>0]\n",
    "stereolib.write_point_cloud(point_cloud_name, xyz_dwn_filtered, rgb_left_dwn_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9f0856-4116-417c-bf35-a8cfdaf46713",
   "metadata": {},
   "source": [
    "### Outputs :\n",
    "\n",
    "Here is the illustration of the efficiency of the 3D filtering\n",
    "\n",
    "<img src=\"static/filtered_pointcloud.png\" width=\"750\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bda6f96-f087-4b4e-a4c3-c6a10373a843",
   "metadata": {},
   "source": [
    "# Step 6 : Estimating the mean height of the plants with a basic method\n",
    "\n",
    "The height of a plant is basically the distance between the ground and its top\n",
    "\n",
    "- By having access to the depth array, we can try to estimate the ground and the top of the canopy distances\n",
    "\n",
    "- By calculating the difference of these distances, we should retrieve the height of the plants\n",
    "\n",
    "<img src=\"static/plant_height.png\" width=\"300\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba0d871-1b17-40ee-a52a-6f5910167712",
   "metadata": {},
   "source": [
    "## 1.  Estimation of the ground depth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e036b-216b-4106-9255-02adbfd26a9e",
   "metadata": {},
   "source": [
    " ### Outputs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1f3d5-a90b-4ae8-961b-ac6ff83339c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the third channel of the (X,Y,Z) filtered downsampled array (float32 values, in mm), corresponding to z values, i.e. depth_image to sensor\n",
    "depth_array = xyz_dwn_filtered[:, 2]\n",
    "\n",
    "# Convert the floating values into rounded integer values (mm precision is more than what is reasonably achievable).\n",
    "depth_array = np.round(depth_array).astype(np.uint32)\n",
    "\n",
    "# Plot the depth_array histogram\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(depth_array) - np.min(depth_array)) / 10.)\n",
    "\n",
    "# We compute the depth_array histogram \n",
    "depth_hist, depth_bins = np.histogram(depth_array, bins=hist_range)\n",
    "#depth_hist, depth_bins = np.histogram(depth_array, bins=10)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution after 3D filtering\")\n",
    "\n",
    "# Plot the histogram\n",
    "plt.plot(depth_bins[1:], depth_hist)\n",
    "\n",
    "# Smooth the histogram to remove micropeaks\n",
    "filtered_depth_hist = gaussian_filter1d(depth_hist, sigma=2)\n",
    "\n",
    "# We normalize the histogram\n",
    "norm_filtered_depth_hist = filtered_depth_hist / np.max(filtered_depth_hist)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution after a light smoothing\")\n",
    "\n",
    "# We plot the filtered depth histogram\n",
    "plt.plot(depth_bins[1:], norm_filtered_depth_hist)\n",
    "#plt.savefig(path_outputs / Path('filtered_depth_hist.png'))\n",
    "\n",
    "# We look for peaks into the depth histogram. In the favourable case (such this one) where soil is not so masked by the vegetation, \n",
    "# we can assume that the last peak corresponds to the ground level.\n",
    "# The function returns peaks indices in the array, if any was found\n",
    "peaks, _ = find_peaks(norm_filtered_depth_hist, height=0.1)\n",
    "\n",
    "# We display the depths corresponding to the peaks indices\n",
    "depth_bins[peaks+1]\n",
    "\n",
    "# We assign the depth of the last peak to our ground estimated value\n",
    "ground_depth = 0\n",
    "if len(peaks) > 0:\n",
    "    ground_depth = depth_bins[peaks[-1]+1]\n",
    "\n",
    "ground_depth = np.round(ground_depth).astype(np.uint32)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# We plot once again the filtered depth histogram\n",
    "plt.plot(depth_bins[1:], norm_filtered_depth_hist)\n",
    "\n",
    "# Margins and limits of the plot\n",
    "ax.margins(x=0, y=0)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# We plot the vertical line corresponding to the ground estimated depth\n",
    "plt.vlines(ground_depth, 0,1, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'{ground_depth}', xy=(ground_depth, 0), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Depth to sensor in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Depth distribution and ground depth estimation\")\n",
    "\n",
    "plt.savefig(path_outputs / Path('filtered_depth_hist.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f164b79-5cfa-4426-a91a-9089f2af1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ground depth estimation = {(ground_depth)} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3936bf60-55ee-4211-9c3a-c8b1defe46c3",
   "metadata": {},
   "source": [
    "### Checking\n",
    "\n",
    "Let's have a look at the information we can retrieve picking a soil point in our pointcloud \n",
    "(Made with CloudCompare)\n",
    "\n",
    "<img src=\"static/read_soil.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba45f55-e463-46b1-8bb9-820d2d52575c",
   "metadata": {},
   "source": [
    "## 2.  Estimation of the mean height of the plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67abb6-7ef9-4e41-9556-8f9ac9e0d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heigths may then be simply estimated by calculating the difference between the ground distance and the distances to objects\n",
    "height_array = ground_depth - depth_array\n",
    "\n",
    "# We ensure that there won't be values out of the expected range\n",
    "height_array = height_array[height_array > 0]\n",
    "height_array = height_array[height_array < ground_depth]\n",
    "\n",
    "# We choose a step of 1 cm, which seems a be a correct compromise considering the acquisition system and context.\n",
    "hist_range = int((np.max(height_array) - np.min(height_array)) / 10.)\n",
    "\n",
    "# We compute the height histogram\n",
    "height_hist, height_bins = np.histogram(height_array, bins=hist_range)\n",
    "\n",
    "# Smooth the histogram to remove micropeaks, just as we did with depth\n",
    "filtered_height_hist = gaussian_filter1d(height_hist, sigma=2)\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Height above ground_depth in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Height distribution\")\n",
    "\n",
    "# We plot the histogram\n",
    "plt.plot(height_bins[1:], filtered_height_hist)\n",
    "plt.savefig(path_outputs / Path('filtered_height_hist.png'))\n",
    "\n",
    "# We compute the cumulative height distribution\n",
    "cumulative_height_distribution = np.cumsum(filtered_height_hist)\n",
    "\n",
    "# We normalize it so that valus are int the interval [0,1]\n",
    "cumulative_height_distribution = cumulative_height_distribution / np.max(cumulative_height_distribution)\n",
    "\n",
    "# We define the desired percentile of the distribution, that will lead to a consistent height value (for wheat), \n",
    "# based on many comparisons with manual height measurements.\n",
    "percentile = 0.98\n",
    "\n",
    "# We keep only height values that are above this percentile\n",
    "mean_height_array, = np.where(cumulative_height_distribution >= percentile)\n",
    "\n",
    "# We retrieve the first value and define it as the mean height\n",
    "mean_height = height_bins[mean_height_array[0]+1]\n",
    "\n",
    "# We create a new figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# We plot the cumulative heights histogram\n",
    "plt.plot(height_bins[1:], cumulative_height_distribution)\n",
    "\n",
    "# We plot the horizontal line corresponding to the desired percentile (98%) of the the height cumulative distribution count \n",
    "plt.hlines(0.98, 0, mean_height, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'0.98', xy=(0, 0.98), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# We plot the vertical line where the desired percentile (98%) is reached in the cumulative distribution\n",
    "plt.vlines(mean_height, 0, 0.98, linestyles='dashed', color='r')\n",
    "\n",
    "# We annotate the line with appropriate value\n",
    "ax.annotate(f'{mean_height:.1f}', xy=(mean_height, 0), xytext=(2, 0),\n",
    "            textcoords='offset points',\n",
    "            ha='left', va='bottom',\n",
    "            fontsize=12, color='red')\n",
    "\n",
    "# Margins and limits of the plot\n",
    "ax.margins(x=0, y=0)\n",
    "ax.set_ylim(0, 1.1)\n",
    "\n",
    "# Name of the horizontal axis\n",
    "ax.set_xlabel('Heights in mm')\n",
    "# Name of the vertical axis\n",
    "ax.set_ylabel('Normalized count of values')\n",
    "\n",
    "# Name of the plot\n",
    "ax.set_title(\"Cumulative heights distribution\")\n",
    "\n",
    "plt.savefig(path_outputs / Path('cumulative_height_distribution.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1ba83c-b060-43c7-a621-cf89f0c52a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Plant mean height estimation = {int(mean_height)} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0ff3fc-c9d0-443b-bbed-555bf6655977",
   "metadata": {},
   "source": [
    "### Checking\n",
    "\n",
    "Let's have a look at the information we can retrieve picking a soil point in our pointcloud \n",
    "(Made with CloudCompare)\n",
    "\n",
    "<img src=\"static/read_height.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f6a14-bacc-4e41-928c-fae4590d1a4b",
   "metadata": {},
   "source": [
    "### Generating the height image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a7ddb-a76f-47ea-9fef-0fa3a6b9a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can go back to the non-filtered depth image to have a representation of the heights of objects in the image, \n",
    "# Distances are in mm in depth_image ; we divide those values by 10, so that we get the heights with a cm precision in a 8 bits image\n",
    "# (values between 0 and 255 cm)\n",
    "height_image = ((ground_depth - depth_image) / 10.)\n",
    "\n",
    "height_image[height_image < 0] = 0\n",
    "height_image[height_image > 255] = 0 \n",
    "\n",
    "height_image = height_image.astype(np.uint8)\n",
    "\n",
    "name_height_image = path_outputs / \"height.png\"\n",
    "\n",
    "stereolib.write_image(name_height_image, height_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445e709d-d3cb-4761-b8c5-562a08615079",
   "metadata": {},
   "source": [
    "### Outputs :\n",
    "\n",
    "Height image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a2e18e-614e-44c3-b090-d7fc328de1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stereolib.display_images([name_height_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df81b6-19e5-4ac6-90d8-21fb5a70db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Height image shape = {height_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac26223-eef1-449d-84df-75f79e081311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8023db-fdfa-4029-9fc7-d24e27910d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
